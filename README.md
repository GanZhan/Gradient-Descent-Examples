# Gradient-Descent-Examples
## A small example for gradient descent algorithms like: BGD、SGD &amp; MBGD
Given bunch of random points (x,y), do the linear regression.    
Using least squares to calculate the loss, and BGD、SGD or MBGD plays the role of optimizer.

<a href="https://www.codecogs.com/eqnedit.php?latex=h(\Theta)&space;=&space;\Theta_1&space;*x_1&space;&plus;&space;\Theta_2*x_2&space;&plus;&space;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\Theta)&space;=&space;\Theta_1&space;*x_1&space;&plus;&space;\Theta_2*x_2&space;&plus;&space;b" title="h(\Theta) = \Theta_1 *x_1 + \Theta_2*x_2 + b" /></a>  
formula above is the function we choose to fit those points.  

<a href="https://www.codecogs.com/eqnedit.php?latex=\jmath&space;(\Theta)=\frac{1}{2m}&space;\sum_{i=1}^{m}{[h_\Theta(x^{i})-&space;y^{i}]^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\jmath&space;(\Theta)=\frac{1}{2m}&space;\sum_{i=1}^{m}{[h_\Theta(x^{i})-&space;y^{i}]^2}" title="\jmath (\Theta)=\frac{1}{2m} \sum_{i=1}^{m}{[h_\Theta(x^{i})- y^{i}]^2}" /></a>  
we use leaset squares to calculate loss.
   
<a href="https://www.codecogs.com/eqnedit.php?latex=h_\Theta(x^{i})&space;=&space;\Theta_1&space;*x_1^i&space;&plus;&space;\Theta_2*x_2^i&space;&plus;&space;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h_\Theta(x^{i})&space;=&space;\Theta_1&space;*x_1^i&space;&plus;&space;\Theta_2*x_2^i&space;&plus;&space;b" title="h_\Theta(x^{i}) = \Theta_1 *x_1^i + \Theta_2*x_2^i + b" /></a>  
 suppose we got m samples, i marked as the i-th sample. <a href="https://www.codecogs.com/eqnedit.php?latex=y^i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y^i" title="y^i" /></a> is the i-th training data for y.  
 
 <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;\jmath&space;(\Theta)}{\partial&space;\Theta_j}&space;=&space;2*\frac{1}{2m}&space;\sum_{i=1}^{m}{[h_\Theta(x^{i})-&space;y^{i}]*x_j^i}=\frac{1}{m}&space;\sum_{i=1}^{m}{[h_\Theta(x^{i})-&space;y^{i}]*x_j^i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;\jmath&space;(\Theta)}{\partial&space;\Theta_j}&space;=&space;2*\frac{1}{2m}&space;\sum_{i=1}^{m}{[h_\Theta(x^{i})-&space;y^{i}]*x_j^i}=\frac{1}{m}&space;\sum_{i=1}^{m}{[h_\Theta(x^{i})-&space;y^{i}]*x_j^i}" title="\frac{\partial \jmath (\Theta)}{\partial \Theta_j} = 2*\frac{1}{2m} \sum_{i=1}^{m}{[h_\Theta(x^{i})- y^{i}]*x_j^i}=\frac{1}{m} \sum_{i=1}^{m}{[h_\Theta(x^{i})- y^{i}]*x_j^i}" /></a>  
 formula above is what we use to calculate the gadient for <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta_j" title="\Theta_j" /></a>, we got gradient for two parameters, j=1 for  <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta_1" title="\Theta_1" /></a> , j = 2 for <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta_2" title="\Theta_2" /></a>    
 
 In my example, <a href="https://www.codecogs.com/eqnedit.php?latex=h(\Theta)&space;=&space;\Theta_1&space;*x_1&space;&plus;&space;\Theta_2*x_2&space;&plus;&space;b&space;=&space;w*x&space;&plus;&space;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\Theta)&space;=&space;\Theta_1&space;*x_1&space;&plus;&space;\Theta_2*x_2&space;&plus;&space;b&space;=&space;w*x&space;&plus;&space;b" title="h(\Theta) = \Theta_1 *x_1 + \Theta_2*x_2 + b = w*x + b" /></a> . 
 so we only care about one param <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta_j" title="\Theta_j" /></a> is the <a href="https://www.codecogs.com/eqnedit.php?latex=w" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w" title="w" /></a> here.  
 
 <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;\jmath&space;(\Theta)}{\partial&space;b}&space;=&space;2*\frac{1}{2m}&space;\sum_{i=1}^{m}{[h_\Theta(x^{i})-&space;y^{i}]}=\frac{1}{m}&space;\sum_{i=1}^{m}{[h_\Theta(x^{i})-&space;y^{i}]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;\jmath&space;(\Theta)}{\partial&space;b}&space;=&space;2*\frac{1}{2m}&space;\sum_{i=1}^{m}{[h_\Theta(x^{i})-&space;y^{i}]}=\frac{1}{m}&space;\sum_{i=1}^{m}{[h_\Theta(x^{i})-&space;y^{i}]}" title="\frac{\partial \jmath (\Theta)}{\partial b} = 2*\frac{1}{2m} \sum_{i=1}^{m}{[h_\Theta(x^{i})- y^{i}]}=\frac{1}{m} \sum_{i=1}^{m}{[h_\Theta(x^{i})- y^{i}]}" /></a>  
 formula above is the gradient for param <a href="https://www.codecogs.com/eqnedit.php?latex=b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b" title="b" /></a>   
 
The main diferdence for BGD、SGD & MBGD is the number of samples: <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> in BGD is number of whole training data, <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> in SGD is 1, normally we take one random sample from whole training data, <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> in MBGD is small part of whole training data, it can be 2、4...etc.
